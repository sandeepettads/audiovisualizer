<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Visualizer</title>
    <style>
        body {
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            background-color: #000;
            margin: 0;
            overflow: hidden;
            font-family: Arial, sans-serif;
            color: #fff;
            flex-direction: column;
        }
        canvas {
            border: 1px solid #333;
            background-color: #000;
        }
        button {
            padding: 10px 20px;
            font-size: 1.2em;
            margin-top: 20px;
            cursor: pointer;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 5px;
        }
        button:hover {
            background-color: #0056b3;
        }
        .message {
            margin-top: 10px;
            font-size: 1.1em;
        }
    </style>
</head>
<body>
    <h1>Voice Visualizer</h1>
    <canvas id="audioVisualizer" width="800" height="200"></canvas>
    <button id="startButton">Start Microphone</button>
    <div class="message" id="statusMessage">Click "Start Microphone" to begin.</div>

    <script>
        const startButton = document.getElementById('startButton');
        const statusMessage = document.getElementById('statusMessage');
        const canvas = document.getElementById('audioVisualizer');
        const ctx = canvas.getContext('2d');

        let audioContext;
        let analyser;
        let microphone;
        let animationFrameId;

        startButton.addEventListener('click', async () => {
            if (audioContext && audioContext.state === 'running') {
                stopVisualizer();
                startButton.textContent = 'Start Microphone';
                statusMessage.textContent = 'Visualization stopped.';
                return;
            }

            try {
                // Request access to the user's microphone
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                statusMessage.textContent = 'Microphone access granted. Speaking now!';

                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                microphone = audioContext.createMediaStreamSource(stream);

                analyser.fftSize = 2048; // Fast Fourier Transform size
                const bufferLength = analyser.frequencyBinCount; // Number of data points
                const dataArray = new Uint8Array(bufferLength);

                microphone.connect(analyser);
                // analyser.connect(audioContext.destination); // Connect analyser to speakers if you want to hear yourself

                startButton.textContent = 'Stop Microphone';

                // Drawing the waveform
                function draw() {
                    animationFrameId = requestAnimationFrame(draw);

                    analyser.getByteTimeDomainData(dataArray); // Get waveform data

                    ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear previous frame

                    ctx.lineWidth = 2;
                    ctx.strokeStyle = 'rgb(0, 255, 255)'; // Cyan color for waveform

                    ctx.beginPath();

                    const sliceWidth = canvas.width * 1.0 / bufferLength;
                    let x = 0;

                    for (let i = 0; i < bufferLength; i++) {
                        const v = dataArray[i] / 128.0; // Normalize data to 0-2
                        const y = v * canvas.height / 2; // Scale to canvas height

                        if (i === 0) {
                            ctx.moveTo(x, y);
                        } else {
                            ctx.lineTo(x, y);
                        }

                        x += sliceWidth;
                    }

                    ctx.lineTo(canvas.width, canvas.height / 2);
                    ctx.stroke();
                }

                draw(); // Start the visualization
            } catch (err) {
                console.error('Error accessing microphone:', err);
                statusMessage.textContent = 'Error accessing microphone. Please allow access.';
                startButton.textContent = 'Start Microphone';
            }
        });

        function stopVisualizer() {
            if (microphone) {
                microphone.disconnect();
                microphone.mediaStream.getTracks().forEach(track => track.stop()); // Stop all tracks in the stream
            }
            if (audioContext) {
                audioContext.close();
            }
            if (animationFrameId) {
                cancelAnimationFrame(animationFrameId);
            }
            ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear canvas
        }

        // Handle page unload to ensure microphone is released
        window.addEventListener('beforeunload', () => {
            stopVisualizer();
        });
    </script>
</body>
</html>